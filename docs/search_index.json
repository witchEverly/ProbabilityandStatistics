[["index.html", "Notes for Probability and Statistics Chapter 1 Hello, World! 1.1 Cover 1.2 Contents", " Notes for Probability and Statistics Jessica B 2024-02-15 Chapter 1 Hello, World! This is a book of notes for probability and statistics. Avoid using a mobile browser with the current version. In probability, we solve forwards. It starts with an exact and well-defined rule. We use this rule to predict what will happen in the future. We don’t know the results, but probability allows us to quantify what should happen. In statistics, we solve backwards. It starts with the end results, known as data. We take this data and try to determine what well-defined rules created it. T Janesky 1.1 Cover Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce eu nulla vitae est accumsan consequat. Donec nec purus vel odio cursus efficitur non et justo. Pellentesque vehicula, mauris quis fermentum varius, lectus elit volutpat arcu, eu efficitur erat elit id purus. Sed id nunc arcu. Sed eu orci ligula. Vestibulum sit amet lacinia arcu. Nunc gravida justo id dictum tincidunt. Pellentesque consequat arcu vel mi fermentum, vel varius justo viverra. Maecenas interdum eros vitae lectus tristique, quis facilisis risus hendrerit. Vivamus vel sem vel ligula hendrerit ullamcorper. Ut vel nulla eu ex euismod elementum. Aenean sodales metus eget purus blandit, nec fringilla dolor venenatis. Praesent malesuada vitae eros id scelerisque. Fusce fringilla, purus a mattis lacinia, ipsum urna vestibulum enim, eget efficitur nulla dui id sapien. Integer volutpat felis a est lacinia elementum. Vivamus nec consectetur urna. In lacinia tristique justo a venenatis. Nullam suscipit nec dolor a aliquet. 1.2 Contents Probability of Events Counting Methods Conditional Probability Random Variables &amp; Probability Distributions Special Discrete Distributions Special Continuous Distributions Bivariate Random Variables Estimation "],["probability-of-events.html", "Chapter 2 Probability of Events 2.1 Axioms of Kolmogorov 2.2 Implied consequences of the Axioms 2.3 About Sets, Sample Spaces, and events", " Chapter 2 Probability of Events 2.1 Axioms of Kolmogorov Non-negativity Axiom \\[ \\text{For any event A in our sample space, } \\mathbb{P}(A)≥0 \\] Unity Axiom \\[ \\text{Normalization, also called the assumption of unit measure,}\\\\ \\text{defines the probability of the sample space } \\Omega \\text{ as } \\mathbb{P}(\\Omega)=1 \\] \\(\\sigma\\)-additivity Axiom \\[ \\text{Any countable sequence of disjoint (mutually exclusive) sets satisfies} \\\\ \\mathbb{P}(\\bigcup_{i=1}^{\\infty}E_{i}) = \\sum_{n=1}^{\\infty}\\mathbb{P}(E_{i}) \\qquad\\text{where }\\ E_i\\cap E_j = \\varnothing, \\quad \\forall i\\not=j \\] Additional Notes These axioms allow us to derive the theorems and properties that we frequently come across in the study of probability. Some authors tend to use finite additivity, rather than sigma additivity as the third axiom. The Axioms can be stated in two axioms, rather than three: \\((1) \\qquad \\text{Axiom 1 }\\mathbb{P(\\varnothing) =0, P(\\Omega)=1}, \\\\ (2) \\qquad\\sigma-\\text{additivity}\\) 2.2 Implied consequences of the Axioms Demonstrating these immediate consequences illustrates the power of the third axiom, and its interaction with the remaining two axioms. \\[ \\text{ Probability of the empty set} \\\\ \\mathbb{P}(\\varnothing) = 0 \\tag{Theorem 2.1} \\] \\[ \\text{Finite Additivity} \\\\ \\mathbb{P}(\\bigcup_{n=1}^{N} E_i)=\\sum_{n=1}^{N}\\mathbb{P}(E_i) \\tag{Theorem 2.2} \\] \\[ \\text{Monotonicity} \\\\ A\\subseteq B \\text{ then }\\mathbb{P}(A)\\le\\mathbb{P}(B) \\tag{Theorem 2.3} \\] Understand that \\(A\\subseteq B\\) means that if a occurs, then B must occur. \\[ \\text{The numeric bound} \\\\ 0\\le\\mathbb{P}(E)\\le1, \\qquad\\forall E\\in F \\tag{Theorem 2.3} \\] \\[ \\text{The complement rule} \\\\ \\mathbb{P}(E^c) = \\mathbb{P}(\\Omega-E) = 1 - \\mathbb{P}(E)\\tag{Theorem 2.4} \\] \\[ \\text{Inclusion-Exclusion Principal} \\\\ P(A\\cup B)=P(A) + P(B)-P(AB) \\tag{Theorem 2.5} \\\\ \\] \\[ \\text{Generalized Inclusion-Exclusion Principle} \\\\ \\mathbb{P}(\\bigcup_{i=1}^n A_i) = \\sum_{i=1}^{n} (-1)^{i+1} \\\\ \\sum_{\\Omega \\subseteq} \\mathbb{P}(\\bigcup_{i \\in \\Omega} A_{i}) \\tag{Theorem 2.5} \\] \\[ \\text{Equally Likely Theorem} \\\\ \\mathbb{P}(A)=\\frac{N(A)}{N}\\tag{Theorem 2.7} \\] This is the naive definition of probability \\[ \\text{No Name Theorem} \\\\ \\mathbb{P}(A) = P(AB) + P(AB^c) \\tag{Theorem 2.8} \\] \\[ \\text{Boole&#39;s Inequality} \\\\ \\mathbb{P}(\\bigcup_{i=1}^{\\infty}E_{i}) \\le \\sum_{n=1}^{\\infty}\\mathbb{P}(E_{i}) \\] 2.3 About Sets, Sample Spaces, and events Sets are collections of distinct elements. A sample space is the set of disjoint, collectively exhaustive outcomes taken at a determined level of granularity. A sample space can be described as finite or infinite, discrete or continuous. Sample Space Notation: A sample space can be described with notation below, the long bar, | is read as “such that” \\[ A\\cup B=\\{x| x∈A \\cup x∈B\\} \\\\ A^c=\\{x|x\\not\\in A\\} \\] 2.3.1 Set Notation and LaTex code. Symbol Name LaTeX \\(\\cup\\) Union \\cup \\(\\cap\\) Intersection \\cap \\(\\in\\) Belongs to \\in \\(\\varnothing\\) Null set \\varnothing \\(\\Omega\\), \\(S\\) Universal set \\Omega \\(\\omega\\), \\(s\\) Atom, singleton \\omega \\(\\subset\\) Proper subset \\subset \\(\\subseteq\\) Subset, S is contained within A \\subseteq \\(\\bigcup\\) infinitary union, generalized union, unified union \\bigcup \\(\\bigcap\\) infinitary intersection, generalized intersection, unified intersection cap \\bigcap 2.3.2 Some Set Definitions Name Definition Rule Certainty The probability of the sample space (the set of all possible outcomes) is 1, representing absolute certainty \\(P(\\Omega) = 1\\) Impossibility The probability of an impossible event is 0 \\(P(\\varnothing) = 0\\) Mutual Exclusivity Two events A and B cannot occur simultaneously, in terms of probability \\(P(A \\cap B) = 0\\) Complement The complement of event A is self-defined \\(P(A^c) = 1 - P(A)\\) Sum Rule The probability of the union of two mutually exclusive events A and B \\(P(A \\cup B) = P(A) + P(B)\\) Difference Rule The probability of the difference between two events A and B \\(P(A - B) = P(A) - P(A \\cap B)\\) Note that Impossibility and Certainty are both themselves axioms 2.3.3 Relational Laws Name Law Commutative \\(AB = BA\\) Associative \\((AB)C = A(BC)\\) Distributive \\(AB \\cup C = (A \\cup C)(B \\cup C) ,\\quad A(B\\cup C) = AB \\cup AC\\) Demorgans First Law \\((A \\cup B)^c = A^cB^c\\) Demorgans Second Law \\((AB)^c = A^c \\cup B^c\\) "],["counting-methods.html", "Chapter 3 Counting Methods 3.1 Counting Principal 3.2 Permutations 3.3 Combinations 3.4 Number of Sets in a Subset", " Chapter 3 Counting Methods 3.1 Counting Principal The Fundamental Counting Principle states that the number different possible outcomes for a sequence of \\(n\\) stages is the product of the number of different ways each event can occur. This principal is known by name alternate names such as also the generalized counting principal, the rule of product, the multiplication principal, and the multiplication rule. \\[ \\text{Let }E_1, E_2, ..., E_k \\text{ be sets of events, where each set has }n_1,n_2, ...,n_k \\text{ respective elements.} \\\\ \\text{Then there are }n_1×n_2×n_3×···×n_k \\text{ combinations in which we, first,} \\\\ \\text{choose an element of }E_1, \\text{then choose element of }E_2, \\text{ ... , ending with } E_k. \\] This principal is one of a few Combinatorial principles. The counting principal can be visualized with a tree graph. There are some immediate rules that can be followed from the counting principal. It is fair to say that this intutive rule serves as a building block for introductory combinatorical problems. Below is a table rules that are all derived from the counting principal. Rule Equation Permutations, w/ Replacement \\(n^k\\) Permutations w/o Replacement \\(\\frac{n!}{(n-r)!}\\) Combinations, w/o Replacement \\(\\frac{n!}{r!(n-r)!}\\) Combinations w/ Replacement \\(\\binom{n+r-1}{r}\\) Number of Possible Subsets in a Set \\(2^n\\) 3.2 Permutations A permutation is the number of ways we can order \\(n\\) distinct objects. From the generalized counting principle we know that if we have \\(n\\) ways to make the first selection, then we will have one fewer (\\(n-1\\)) ways to make our next selection, \\(n-2\\) ways to make the third selection, and so on… until we only have one option for the last selection. This results in the factorial equation \\[ (n)(n−1)(n−2)···(1)=n! \\] We use the notation \\(_nP_r\\) to denote the number of permutations of a set \\(A\\) containing \\(n\\) elements taken \\(r\\) at a time where \\(1 ≤ r ≤ n\\) The number of \\(r\\)-element permutations of a set containing \\(n\\) objects is given by \\[ _nP_r = \\frac{n!}{(n-r)!} \\] Note: To avoid division by zero when \\(n=r\\) we define \\(0! = 1\\) To derive the formula for \\(_nP_r\\) we consider the number of choices for each selection. This is the same things as the factorial equation above, only with one small difference. We are now considering an \\(r\\)-element permutation, instead of an \\(n\\)-element permutation. In this case, the \\(r\\)th outcome can take on \\(n − (r − 1)\\) possible choices, as opposed to the case before where we had only one option for the last selection. _____ _____ _____ ... _____ (n) (n-1) (n-2) ... (n-(r-1)) We accounting for the ways you can permeate \\(r\\) objects, the result \\(n - (r - 1)\\) is the number of objects taken into account. This leads us to the equation \\[ _nP_r =n(n−1)(n−2)···(n−r+1) \\] Aside: Permutations with \\(_nP_n\\) equate to \\[ _nP_n =n(n−1)(n−2)···(n−n+1)=n! \\] Manipulating the equation \\(n(n−1)(n−2)···(n−r+1)\\) leads us to the general formula \\[ _nP_r = \\frac{n!}{(n-r)!} \\] 3.2.1 Permutations with Replacement By applying the counting principle, we can observe that the number of permutations in replacement sampling is equivalent to \\[ n^k = (n)(n)....(n_k) \\] 3.2.2 Permutations with distinguished elements The formulas above are valid only if all of the objects of the set are distinguishable from each other. If there are repeated elements then we must divide to cancel out the permutations that result from repeated elements like so: \\[ \\frac{n!}{n_1! × n_2 ! × · · · × n_k!} \\] Consider the names Brungard and Karapanian. If for some reason we wanted to know the possible ways to rearrange the letters of each of word we would have Brungard \\[ \\frac{8!}{2!} = 20,160 \\] Karapanian Here are ten letters, but only six are distinct \\[ \\frac{10!}{4! *2!}= 75,600 \\] While there are still more ways to permute the longer last name, consider that if you did not account for repeated characters, the overcount will exceed three million. factorial(10) - factorial(10)/(factorial(4)*factorial(2)) [1] 3553200 3.3 Combinations we have for \\(r \\le n\\) \\[ \\binom{n}{r}=\\frac{n!}{r!(n-r)!} \\] We are calculating the number of \\(r\\)-element subsets from a \\(n\\)-element set, where order matters. This is the number of all r-element combinations of n objects, where we only use valid non-negative integers (e.g. 0,1,2, …) where \\(r \\le n\\). [need add derivation] 3.3.1 Combinations with replacement \\[ \\binom{n+k-1}{k} \\] 3.3.2 Useful Relations Choosing all and choosing nothing There is only one combination to choose all elements in a set. Similarly, there is only one way to choose nothing. \\[ \\binom{n}{n}=1 \\qquad \\&amp; \\qquad \\binom{n}{0}=1 \\] Both relations can be easily be shown algebraically \\[ \\binom{n}{n}=\\frac{n!}{n!(n-n)!}=1 \\] Choosing 1 from n objects is the same as choosing n-1 fron n objects \\[ \\binom{n}{1}=\\binom{n}{n-1} \\] consider this generalization where \\(0 \\le r \\le n\\) \\[ \\binom{n}{r}=\\binom{n}{n-r} \\] and \\[ \\binom{n+1}{r}=\\binom{n}{r}+\\binom{n}{r-1} \\] 3.4 Number of Sets in a Subset The set of all subsets of A is called the power set of A. We can calculate the number of subsets in a set as \\(2^n\\), because each element can be either included or excluded (hence the two options) "],["conditional-probability.html", "Chapter 4 Conditional Probability 4.1 Conditional Probability 4.2 Law of Multiplication 4.3 Bayes’ Formula 4.4 Monty Hall Problem, Explained:", " Chapter 4 Conditional Probability Conditional probability is not a theorem, nor a conjecture. It is not a lemma or a corollary. It aims to answer the question of how you should update probability based on newfound evidence. It is philosophically intuitive idea that the conditional probability \\(\\mathbb{P}(A|B)\\) gives an advantage, when knowing the occurrence of \\(B\\) changes the occurrence of \\(A\\). 4.1 Conditional Probability The probability of \\(\\mathbb{P}(A|B)\\) is given by the ratio of the relative frequency of the intersection of \\(A\\) and \\(B\\) and the relative frequency of \\(B\\). We can interpret this as the ratio of the probability of joint occurrence of \\(A\\) and \\(B\\) and the probability of \\(B\\). We define conditional probability as \\[ \\mathbb{P}(A|B)=\\frac{\\mathbb{P}(AB)}{\\mathbb{P}(B)} \\] where \\(\\mathbb{P}(B)&gt;0\\). Set Theory Interpretation: \\(P(AB)\\) is the intersection of common elements between sets \\(A\\) and \\(B\\). Joint occurrence better describes the probability that events are to occur. Conditional Probability is a Probability Measure That is, they satisfy the same axioms that ordinary probabilities satisfy. This enables us to use the theorems that are true for probabilities for conditional probabilities as well. Reduction of a Sample Space It is possible to reduce a sample space e.g. from (S to B) and have a smaller set of subsets. Making it easier to calculate conditioned probabilities. 4.2 Law of Multiplication Understand that conditional probability represents the relation between \\(\\mathbb{P}(A|B)\\) and \\(\\frac{\\mathbb{P}(AB)}{\\mathbb{P}(B)}\\) where \\(\\mathbb{P}(B) &gt; 0\\). By simply multiplying both sides by the \\(\\mathbb{P}(B)\\), The law of multiplication demonstrates that probability of the joint occurrence of A and B is the product of the probability of B and the conditional probability of A given that B has occurred. \\[ \\mathbb{P}(AB)=\\mathbb{P}(A|B)\\mathbb{P}(B), \\text{ where } \\mathbb{P}(B)&gt;0 \\] also, \\[ \\mathbb{P}(AB)=\\mathbb{P}(BA)=\\mathbb{P}(B|A)\\mathbb{P}(A), \\text{ where } \\mathbb{P}(A) &gt; 0 \\] We can also calculate the joint probability of three events by using association laws. Generalizing these theorems stated can be extended to \\(n\\) events to \\(k\\) events, the resulting formula can be proved by mathematical induction. \\[ \\text{If } \\mathbb{P}(A_1 A_2 A_3 …A_{n−1}) &gt; 0 \\text{, then} \\\\ \\mathbb{P}(A_1 A_2 A_3 …A_{n−1}A_n) = \\mathbb{P}(A_1)P(A_2 | A_1)P(A_3 | A_1A_2)···(A_n | A_1A_2A_3 ···A_{n−1}) \\] ##Law of Total Probability \\[ \\mathbb{P}(A) = \\mathbb{P}(A|B)\\mathbb{P}(A) + \\mathbb{P}(A|B^c)\\mathbb{P}(B^c) \\] Generalized Law of Total Probability \\[ \\text{If }\\mathbb{P}(A_1 A_2 A_3 …A_{n−1}) \\&gt; 0 \\text{, then} \\\\ \\mathbb{P}(A_1 A_2 A_3 …A_{n−1}A_n) = \\mathbb{P}(A_1)P(A_2 \\| A_1)P(A_3 \\| A_1A_2)···(A_n \\| A_1A_2A_3 ···A_{n−1}) \\] 4.3 Bayes’ Formula Bayes’ Rule is a simple calculation but a big idea about beliefs. Consider that the partitioned sample space we imagine for the law of total probability. With Bayes’ Rule we are update our belief about something given that we know some other event is true or has occurred. We want to calculated the probability of an unknown event occurring. examining the hypothesis given evidence is true we arrive at the expression \\[ P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)} \\] \\(P(H|E)\\) represents the probability of hypothesis \\(H\\) given evidence \\(E\\). \\(P(E|H)\\) denotes the probability of observing evidence \\(E\\) given the hypothesis \\(H\\). \\(P(H)\\) is the prior probability of hypothesis \\(H\\) \\(P(E)\\) represents the probability of observing evidence \\(E\\). In order to update our beliefs… we need to consider our prior knowledge \\(P(H)\\) 4.4 Monty Hall Problem, Explained: This assumes you know the background, i.e. “rules” of the Monty Hall problem https://en.wikipedia.org/wiki/Monty_Hall_problem We begin with a 1/3 chance of choosing a door which has a the car. “After the host opens a door there is a 1/2 chance of finding the car” We may conclude this if we use the equally likely theorem and reducing the same space with conditional probability. This conclusion falsely assumes independence; that Monty has a choice between the two doors, i.e. he is choosing between them at random. The issue is that this does not constitute the full conditioning of the evidence provided to us. We possess additional knowledge that the door selected was door two. In the event that we switch doors, we can reason that between the originally picked door (e.g. door 1), the closed door is now paired with one that we know is a goat (doors 2 and 3) What does this mean; it means that before conditioning we had a one third chance of door 1 having the car, the complement is 2/3 and after conditioning we know that door between door two and three one of them must have a must have the car, which has a probability of 2/3. "],["probability-distributions-part-1.html", "Chapter 5 Probability Distributions (Part 1) 5.1 Random Variables 5.2 Distribution Functions 5.3 Calculating Probability 5.4 Probability Mass Functions 5.5 Probability Density Functions 5.6 Finding the value of c, a constant in a distribution.", " Chapter 5 Probability Distributions (Part 1) 5.1 Random Variables The concept of random variables allows us to explore what is unknown, or governed by randomness. We define a random variable as a real-valued function that maps any given event/outcome of a random experiment to a real number. The most formal, axiomatic definition of a random variable involves measure theory. \\[ X:\\Omega\\rightarrow \\mathbb{R} \\] The outcome of a coin toss can be represented by a random variable that takes the value 1 if the coin lands heads and 0 if it lands tails. Depending on the type of data, a random variable is either discrete or continuous. We can have several random variables defined on a sample space. Support Set An event/outcome of a random experiment that is a subset from our sample space. The support set is the set of possible values of \\(X\\). A random variable is not the same as it’s probability distribution: A probability distribution is more of a blueprint for all of the possible values that a random variable can take on, i.e. the support. There are many different mistakes that can be made by thinking random variable(s) abide by the same rules as something like a PMF. 5.2 Distribution Functions Also called, Cumulative Distribution Functions. A distribution function characterizes a random variable, they are used as a way to encode information about a random variable. More formally, a random variable \\(X\\) is a function \\(F\\) from \\((-\\infty, +\\infty)\\) to \\(\\mathbb{R}\\) defined by\\(F(t)=P(X \\le t)\\). Properties From the definition of a distribution function, the following properties are determined: \\(F(t)\\) is non-decreasing \\(F(t)\\) is right continuous Satisfies \\(\\lim_{t \\rightarrow \\infty}= 1\\) Satisfies \\(\\lim_{t \\rightarrow -\\infty}= 0\\) If a function satisfies these properties then it is a distribution function of a random variable. \\[ F_X(t) = P(X \\le t) = \\sum_{\\{x \\in R_x | x\\le t\\}} p_X(x)\\tag{Discrete} \\] \\[ F_X(t) = P(X \\le t) = \\int_{-\\infty}^{t} f_X(x) dx \\tag{Continuous} \\] Additional Notes For x-values where \\(F&#39;(X)\\) exists, \\(F&#39;(X) = f(x)\\) For discrete random variables, CDF’s are step functions. A distribution function does not need to be continuous at all points, just right continuous. 5.3 Calculating Probability Recall that the CDF can characterize a random variable, how is this done? Considered the events that are represented as the following inequalities, if \\(\\mathbb{P}(X \\le t)\\) is known for all \\(t \\in \\mathbb{R}\\) all of the following events can be calculated: \\[ (X \\le a) \\qquad (X &lt; a) \\qquad (X \\ge a) \\qquad (X &gt; a) \\qquad (X = a) \\] \\[ (a \\le X \\le b) \\qquad (a &lt; X &lt; b) \\qquad (a &lt; X \\le b) \\qquad (a \\le X &lt; b) \\] Discrete Case The CDF is defined as \\(F(X) = P(X \\le x)\\). Use these relations to find the following probabilities Probability of Event CDF Calculation \\(P(X \\ge a)\\) \\(1 - F(a-)\\) \\(P(X &gt; a)\\) \\(1 - F(a)\\) \\(P(X &lt; x)\\) \\(F(a-)\\) \\(P(X = a)\\) \\(F(a)-F(a-)\\) \\(P(a &lt; X \\le b)\\) \\(F(b) - F(a)\\) \\(P(a \\le X &lt; b)\\) \\(F(b-) - F(a-)\\) \\(P(a \\le X \\le b)\\) \\(F(b) - F(a-)\\) \\(P(a &lt; X &lt; b)\\) \\(F(b-) - F(a)\\) Continuous Case The nature of continuous data makes calculating probabilities using the CDF much simpler. This is because of the way by which we integrate to calculate these continuous probabilities. \\[ P(a&lt;X&lt;b)=P(a\\leq X&lt;b)=P(a&lt;X\\leq b)=P(a\\leq X\\leq b)=\\int_{a}^{b} f(t) \\, dt \\] Probability of Event CDF Calculation \\(P(X = x) = 0\\) \\(F(x) =0\\) \\(P(a &lt; X \\leq b), \\\\ P(a \\leq X &lt; b), \\\\ P(a \\leq X \\leq b), \\\\P(a &lt; X &lt; b)\\) \\(F(b) - F(a)\\) \\(P(X &gt; a)\\) \\(1 - F(a)\\) \\(P(X \\geq a)\\) \\(1 - F(a)\\) \\(P(X &lt; a)\\) \\(F(a)\\) 5.4 Probability Mass Functions Also called a PMF, probability function, or discrete probability function. Defined as a real-valued function from support set of a random variable \\(X\\) to \\(\\mathbb{R}\\), i.e. \\(p: \\mathbb{R_x} \\rightarrow \\mathbb{R}\\). \\[ p_X(x) = P(X=x) = P(\\{\\omega \\in \\Omega | X(\\omega)=x\\}) \\] a proper PMF satisfies the properties \\[ p(x) \\ge 0 \\] and \\[ \\sum_{x \\in R_x}p(x)=1 \\] 5.5 Probability Density Functions Defined as a real-valued function from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\), i.e. \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) a proper PDF satisfies the following properties \\[ \\tag{1} f(x) \\ge 0, \\quad \\forall x \\in \\mathbb{R} \\] and \\[ \\int_{-\\infty}^{\\infty}f(x)dx=1 \\tag{2} \\] Additional notes (PMF and PDF) The PMF is defined as the difference between consecutive CDF values The properties of the PMF and PDF show that they are probability measures, as shown by the Axioms of Kolmogorov. 5.6 Finding the value of c, a constant in a distribution. When we are asked to find a constant value in order to define a function, This definition of a probability distribution to set the PMF or PDF (which ever applies) equal to 1. We then solve for the constant value. What does this tell us? I means that if that constant was any other value.. then the function in question would no longer be defined as a PDF or CDF. "],["probability-distributions-part-2.html", "Chapter 6 Probability Distributions (Part 2) 6.1 Expected Value 6.2 Variance 6.3 Law of the Unconscious Statistician (LOTUS) 6.4 Moments 6.5 Transformations of Random Variables", " Chapter 6 Probability Distributions (Part 2) We cover measures of central tendency like expected value, and measures spread like variance because they allow us to recognize characteristics of a distribution. These are called the parameters of a distribution. 6.1 Expected Value \\[ E[X] = \\sum_{x \\in S} xp(x) \\tag{Discrete} \\] \\[ E[X] = \\int_{S}{xf(x)}dx \\tag{Continuous} \\] Properties of Expected Value \\[\\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]\\] \\[\\mathbb{E}[c] = c \\quad \\text{(where } c \\text{ is a constant)}\\] \\[\\mathbb{E}[X_1 + X_2 + \\ldots + X_n] = \\mathbb{E}[X_1] + \\mathbb{E}[X_2] + \\ldots + \\mathbb{E}[X_n]\\] Additional Notes What if the discrete set is infinitely countable? The sum needs to converge absolutely for E(X) to exist. Notation: \\(\\mathbb{E}[X]\\) and \\(\\mu_x\\) can be used interchangeably When deriving properties of expectation, it is crucial to understand the \\(\\mathbb{E}[X]\\) is a fixed constant. 6.2 Variance The variance of a random variable is the expected value (i.e. the average) of the squared deviation from the mean. \\[ \\\\ Var(X) = \\sum_{x \\in A}(x-\\mu)^2p(x) = \\mathbb{E}([X − \\mathbb{E}[X]^2) \\tag{Discrete} p(x) \\] \\[ Var(x) = \\int_{-\\infty}^{\\infty}{\\sum_{x \\in A}(x-\\mu)^2}f(x)dx = \\int_{-\\infty}^{\\infty}{\\mathbb{E}([X − \\mathbb{E}[X]^2)}dx \\tag{Continuous} \\] Shortcut formula for variance \\[ \\mathbb{E}[X^2]-\\mathbb{E}[X]^2 \\] Properties of Variance \\[\\text{Var}(aX + bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y) + 2abCov(X,Y)\\] \\[ \\text{Var}(aX + b) = a^2\\text{Var}(X) \\] \\[\\text{Var}(c) = 0 \\quad \\text{(where } c \\text{ is a constant)}\\] Additional Notes Properties of variance tend to be derived from expectation It is implicit that expectation of \\(X\\) is defined, however even if \\(E(X)\\) exists it is possible that \\(Var(X)\\) is infinite. If independent, the covariance is \\(0\\). Variance is quadratic. 6.3 Law of the Unconscious Statistician (LOTUS) LOTUS shows the relation between expected value of random variable \\(X\\) and how we can use it to calculate \\(E[g(x)]\\). \\[ E[g(X)] = \\sum_{x \\in R_x} g(x)p(x) \\] \\[ \\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) \\cdot f_X(x) \\, dx \\] Additional Notes Implies expected value of a transformed can be found using the original random variable, i.e. there is no need to compute the transformed CDF or PDF to find it’s expected value. Implies linearity of expectation 6.4 Moments Moments are used to describe the shape of a random variable distribution, moments are quantitative measures. The first moment is expected value The second central moment is variance The standardized third central moment of \\(X\\) is skew Kurtosis is a measure that is based on the fourth moment and the variance of \\(X\\). Additional Notes The existence of higher moments implies the existence of lower moments \\(E[X^n]\\) - \\(n\\)th moment of \\(X\\) 6.5 Transformations of Random Variables If we know a given a random variable, we may want to understand how we can use how can we form new random variables. This is transformation. If we know the mean and standard deviation of the original distributions, we can use that information to find the mean and standard deviation of the resulting distribution. Consider adding a constant to a random variable. The mean would change, but variance would be unaffected. The distribution simply shifts by the amount of which was added to it. \\[ Y = X +k \\\\ \\mu_Y = X + k \\\\ \\sigma^2_Y = \\sigma^2_X \\] Next, consider multiplying by a constant and a random variable. This scales the mean and the variance by the factor. Understand that the overall area of the distribution can not change, as this would effect the definition of our PMF or PDF. So if you stretch some area horizontally by a factor of 3, it would triple the area. Therefore you should compress the area vertically by the same amount in order to get the same area you started with. For example, if a random variable \\(X\\) which has f(x) values: 1,2,3,4,5. If we scale \\(X\\) by 2, we have a Y = 2X, which has the f(y) values 2,4,6,8,10. E(Y) is 6. \\[ Y = kX \\\\ \\sigma^2_Y = k\\sigma^2_X \\\\ \\mu_Y = k\\mu_X \\] "],["special-discrete-distributions.html", "Chapter 7 Special Discrete Distributions 7.1 Bernoulli Random Variables 7.2 Binomial Random Variables 7.3 Geometric Random Variables 7.4 Negative Binomial Random Variables 7.5 Hypergeometric Random Variables 7.6 Poisson Random Variables", " Chapter 7 Special Discrete Distributions 7.1 Bernoulli Random Variables \\[ X \\sim Bern(p) \\qquad R_x = \\{0,1\\} \\] A special case of the binomial distribution for a single trial, i.e., \\(n=1\\). Bernoulli trial is an experiment with two possible outcomes. A Bernoulli random variable itself uses these two possible outcomes, namely success and failure where the random variable takes the values of these outcomes and defines them to be \\(X(\\text{Success}) = 1\\) and \\(X(\\text{Failure}) = 0\\), respectively. PMF \\[ \\begin{equation} p(x) = P(X = x)= \\begin{cases} p &amp; \\text{if } x = 1\\\\ 1 - p = q &amp; \\text{if } x = 0\\\\ 0 &amp; \\text{if } x \\not\\in R_x \\end{cases} \\end{equation} \\] Parameter Details \\[ 0 \\le p \\le 1 \\] Spread / Dispersion Equations \\[ E(X) = p \\qquad Var(X) = p(1-p) \\qquad \\sigma_x = \\sqrt{p(1-p)} \\] Additional Notes A Bernoulli random variable is an indicator function. Alternative expressions for the Bernoulli PMF include: \\[ p(x)=\\mathbb{P} (X = x) = p^x (1-p)^{(1-x)} \\quad x \\in \\{0,1\\} \\] Bernoulli PMF takes on two discrete values, and it jumps abruptly from one value to the other at the point where the value of the random variable changes from 0 to 1. This can be shown graphically with its CDF. This is similar to other discrete random variables. Bernoulli can be generalized to more than 2 outcomes. 7.2 Binomial Random Variables \\[ X \\sim Binom(n,p) \\quad R_x = \\{0,1,2,...,n\\} \\] A binomial random variable is used for determining the number of successes in a fixed number of i.i.d. Bernoulli trials, which is the sum of independent Bernoulli trials **/indicator random variables** \\(X = X_1 + X_2 + X_3 + X_4 + ... + X_n\\) i.i.d. means that each of the random variables that we sum to find \\(X\\) (as shown above) are independent, and have the same distribution (which in this case is \\(X_n ~ Bern(p)\\)), all of which have the same fixed probability of success. PMF \\[ \\begin{equation} p(x) = P(X= x) = \\begin{cases} \\binom{n}{x}p^x(1-p)^{n-x} &amp; \\text{if } x = 0,1,2,...,n\\\\ 0 &amp; \\text{if } x \\not\\in R_x \\end{cases} \\end{equation} \\] Parameter Details \\(p\\): probability of success \\(n\\): number of trials Parameters \\(n\\) and \\(p\\) represent the number of successes in \\(n\\) independent Bernoulli trials, where each trial has a fixed probability of success, denoted by \\(p\\). Spread / Dispersion Equations \\[ E(X) = np \\quad Var(X) = np(1-p) \\quad \\sigma_x = \\sqrt{np(1-p)} \\] Additional Notes Flipping a coin 10 times and counting the number of heads would be an example of a binomial random variable. Calculating where X = 3 (or any value of x in the support) would be a result obtained from the random variable. 7.3 Geometric Random Variables \\[ X \\sim Geom(p) \\qquad R_x = \\{1,2,3,...\\} \\] A geometric random variable is used to find the number of trials that are needed to get the first success in a Bernoulli process (a sequence of *iid* Bernoulli trials is called a Bernoulli process). In other words, a geometric random variable represents the number of failures that occur before the first success in a sequence of independent Bernoulli trials. PMF \\[ \\begin{equation} p(x) = P(X= x) = \\begin{cases} p(1-p)^{x-1} &amp; \\text{if } x = 1,2,...,n\\\\ 0 &amp; \\text{if } x \\not\\in R_x \\end{cases} \\end{equation} \\] Parameter Details 0 &lt; p &lt; 1 p each obs the same probability of success, namely p Spread / Dispersion Equations \\[ E(X) = \\frac{1}{p} \\qquad Var(X) = \\frac{1-p}{p^2} \\quad \\sigma_x = \\frac{\\sqrt{1-p}}{p} \\] Additional Notes Note that Bernoulli random variable is a single experiment, binomial is n experiments, and a geometric is infinitely many experiments. There is a case where there are infinite trials without success. Often used to model situations where you repeatedly perform a binary experiment. Holds the Memoryless Property: “independent trials do not have a memory”, considering what happens upon conditioning a geometric random. Applies to the geometric distribution as \\(P( X &gt; a + b | x &gt; a ) = P ( x &gt; b )\\) \\(E(X)\\) is obviously the average number of trials– knowing the past does not affect the future. The distribution can also be used to denote the number of failures before the first success, in which case the support would obviously begin at 0 instead of 1, and the PMF would look also look a bit different. 7.4 Negative Binomial Random Variables \\[ X \\sim Negative Binom(r,p) \\qquad R_x = {r, r + 1, r + 2, r + 3, ...} \\] A negative binomial random variable is a generalization of a geometric random variable, where \\(X\\) represents the number of trials/experiments until the \\(rth\\) success occurs Parameter Details r, p PMF \\[ p(x; r,p) = P(X= x) = \\binom{x-1}{r-1}p^r(1-p)^{x-r}, \\qquad 0 &lt; p &lt; 1, \\qquad x = r, r + 1, r + 2, ... \\] Spread / Dispersion Equations \\[ E(X) = \\frac{r}{p} \\qquad Var(X) = \\frac{r(1-p)}{p^2} \\quad \\sigma_x = \\frac{\\sqrt{r(1-p})}{p} \\] Additional Notes many alternative formulations aka the pascal distribution 7.5 Hypergeometric Random Variables \\[ X \\sim HyperGeometric(K, N, n) \\qquad R_x = \\{0,1,2,...\\} \\] n are drawn at random and without replacement. PMF \\[ P_X(k)= P(X = k) = \\frac{{K \\choose k} {N-K \\choose n-k}}{N \\choose n} \\] Parameter Details \\(N\\) is the total population size \\(K\\) is the number of individuals in the population that have the attribute of interest \\(n\\) is the sample size \\(k\\) is not a parameter, it is the number of individuals in the sample that have the attribute of interest Spread / Dispersion Equations \\[ E(k)=\\frac{nK}{N} \\qquad Var(k)=\\frac{nK}{N}\\frac{N-K}{N}\\frac{N-n}{N-1} \\] \\[ E(k) = \\sum_{i=1}^{n} k_i P(k_i) \\] \\[ Var(k) = \\sum_{i=1}^{n} (k_i - E(k))^2 P(k_i) \\] Additional Notes 7.6 Poisson Random Variables \\[ X \\sim Pois(\\lambda) \\quad R_x = \\{0,1,2,...\\} \\] A Poisson random variable can be used to approximate a binomial random variable if \\(n\\) is large and \\(p\\) is small. (e.g. lottery tickets sold and winner tickets). This is because a Poisson probability mass function is the limit of a binomial probability mass function. Aside from approximating the binomial distribution, The Poisson distribution appears in connection with the study of sequences of random events occurring over time. Where the number of events occur in a fixed time interval, events occur independently and with a constant rate. PMF \\[ \\begin{equation} p(x) = p(X=x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}, \\quad x = 0,1,2,... \\end{equation} \\] Parameter Details \\[ \\lambda \\in (0,\\infty) \\quad,\\quad \\lambda = np \\] Spread and Dispersion \\[ E(X) = np = \\lambda \\qquad Var(X) = (\\lambda + \\lambda^2) - \\lambda^2 = \\lambda \\qquad \\sigma_x = \\sqrt{\\lambda} \\] Additional Notes \\(np \\le 10\\) and \\(p &lt; 0.1\\) Poisson process is a stochastic process that models the arrival of events over time. When used as an approximation, events can have weak dependence. "],["special-continuous-distributions.html", "Chapter 8 Special Continuous Distributions 8.1 Continuous Uniform Random Variables 8.2 Exponential Random Variables 8.3 Gaussian Random Variables", " Chapter 8 Special Continuous Distributions 8.1 Continuous Uniform Random Variables \\[ X \\sim Unif(a,b) \\] \\[ \\begin{equation} f(x) = \\begin{cases} \\frac{1}{b-a} &amp; \\text{if } a \\le x \\le b\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\end{equation} \\] \\(X\\) is uniformly distributed over the interval \\([a,b]\\), so at all points we have the same height for the PDF. Any two intervals of the same length can occur with equal probability. Center and Dispersion \\[ E(X) = \\frac{a+b}{2} \\qquad Var(X) = \\frac{(b-a)^2}{12} \\qquad \\sigma_x = \\frac{b-a}{\\sqrt{12}} \\] Additional Notes There is also a discrete uniform random variable which is similarly defined. A piece-wise constant pdf has a total area of 1, but can have discontinuities. 8.2 Exponential Random Variables Models the inter-arrival time as a random variable, or amount of time until an event occurs. \\[ X \\sim Exponential(\\lambda) \\\\ \\text{or} \\\\ X \\sim Exponential(\\theta = \\frac{1}{\\lambda}) \\] \\[ \\begin{equation} f_X(x) = F&#39;(x) = \\begin{cases} \\lambda e^{-\\lambda x} &amp; \\text{if } x \\ge 0 \\\\ 0 &amp; x &lt; 0 \\end{cases} \\end{equation} \\] Center and Dispersion \\[ E(X) = \\sigma_x = \\frac{1}{\\lambda} \\qquad Var(X) = \\frac{1}{\\lambda^2} \\] Additional Notes Memoryless property Relationship between Exponential and Geometric \\[ \\begin{equation} f_X(x) = F&#39;(x) = \\begin{cases} \\lambda e^{-\\lambda x} &amp; \\text{if } x \\ge 0 \\\\ 0 &amp; x &lt; 0 \\end{cases} \\end{equation} \\] 8.3 Gaussian Random Variables 8.3.1 Standard Normal Random Variable A special case of the normal distribution that is used to calculated probabilities for any normal distribution, without having to calculate using it’s PDF. \\[ Z\\sim N(0,1) \\] \\[ f(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp [-\\frac{z^2}{2}] \\] \\[ F(z) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^z\\exp [\\frac{-t^2}{2}]dt \\] Center and Dispersion \\[ E(X) = 0 \\qquad Var(X) = 1 \\] Additional Notes We standardize 8.3.2 Normal Random Variable \\[ X \\sim N(\\mu, \\sigma^2) \\] \\[ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp [-\\frac{(x-\\mu)^2}{2\\sigma^2}] \\qquad -\\infty &lt; x&lt;\\infty \\] Center and Dispersion \\[ E(X) = \\mu \\qquad Var(X) = \\sigma^2 \\] Additional Notes "],["bivariate-random-variables.html", "Chapter 9 Bivariate Random Variables 9.1 Independence and Correlation of Two RVs 9.2 Correlation 9.3 Properties of Covariance", " Chapter 9 Bivariate Random Variables Before examining the joint distribution of several random variables, focus first on the bivariate case. The aim is to investigate how two random variables change together through an analysis of their joint distribution. We can derive the marginal distribution of each individual random variable from their joint distribution. If two random variables X and Y are defined on the same sample space, and we call their respective support sets A and B then the function that is the joint distribution is given by \\(f(x,y)= \\mathbb{P}(X=x, Y=y)\\). It is clear that for a bivariate distribution, that \\[ \\int_{A}\\int_{B}f(x,y)dxdy=1 \\qquad \\text{and} \\qquad \\sum_{x \\in A}\\sum_{y \\in B}p(x,y)=1 \\] 9.1 Independence and Correlation of Two RVs When studying the relationship between two random variables, there are two primary characteristics we are interested in– namely, independence and correlation. Independence measures a relationship between X and Y, while correlation measures a linear relationship between X and Y. 9.1.1 Marginal distributions to determine independence Two random variables are independent if and only if the product of their marginal distributions is equal to the joint distribution. \\[ f(x,y) = f(x) \\cdot f(y) \\] 9.1.2 Marginal expectation to determine correlation Two random variables are uncorrelated if their covariance, or equivalently their correlation (the standardized covariance) is a value that is close to zero \\[ E[XY] = E[X] \\cdot E[Y] \\] 9.2 Correlation Correlation is a measure of the strength of a linear relationship. Two variables are not correlated if their correlation equals zero (covariance = 0) if you have independence you know they are uncorrelated. but uncorrelated does not mean they are independent 9.3 Properties of Covariance Derived from expected value, which is a linear operator. \\[ Cov(x,y) = E[XY] - E[X]E[Y] \\] independence is stronger than correlation Additional notes: In bivariate normal distribution, no correlation between variables actually implies independence. "],["estimation.html", "Chapter 10 Estimation 10.1 Estimation from Normal Distribution 10.2 Estimation without Normal Association", " Chapter 10 Estimation When we have a random sample, we make an assumption about the true distribution that the population which from which the sample follows. We can use a point estimate, which we formally refer to as a statistic to estimate parameter values of an assumed distribution. 10.1 Estimation from Normal Distribution If we have \\(n\\) random variables from a population that follows the form \\[ X_i \\stackrel{iid}\\sim N(\\mu_i, \\sigma_i^2) \\qquad i = 1,...,n \\] Then a sample which we assume comes from that distribution, can be described by the following statistics, which help us describe the sample/population. Distribution for sum of any linear combination \\(Y\\) is also normal, with the same mean and variance: \\(Y \\sim N(\\Sigma a_i\\mu_i, \\Sigma a_i^2 \\sigma_i^2)\\) Sample distribution mean of this linear combination is also normal, and has the same mean, with a sample variance that is corrected for bias. \\(\\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})\\) Sampling distribution for the variance is a Chi-squared Distribution \\(\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2(n-1)\\) Sampling distribution for estimating the true \\(\\mu\\) and \\(\\sigma^2\\) (when it is not known), we use a Student’s t Distribution. It is a standard normal divided by the square root of a Chi-squared distribution. \\(\\frac{\\bar{X}-\\mu}{\\frac{s}{\\sqrt{n}}} \\sim t(n-1)\\) 10.2 Estimation without Normal Association How do we estimate parameters without the assumption that the data comes from a normal distribution? We use the central limit theorem When we do not know if a random sample (\\(X_1, X_2,..,X_n\\)) is normally distributed we can use the CLT to conclude that the distribution of the sample mean is also normal. For any distribution the sample mean \\(\\bar{X}\\) has a mean \\(\\mu\\), and sample variance \\(\\frac{\\sigma^2}{n})\\). We use the CLT to approximate (using many samples) that the distribution of the sample will converge to a normal distribution. When \\(n\\) is large enough, we can assume that the distribution of sample is normally distributed. We often hear that the rule is that if \\(n &gt; 30\\) the distribution of \\(\\bar{X}\\) will be approximately normal, however the rate of convergence depends on the original distribution. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
