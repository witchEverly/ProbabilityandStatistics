# Probability Distributions (Discrete and Continuous)

## Probability Mass Functions

Also called a PMF, probability function, or discrete probability function.

Defined as a real-valued **function** from support set of a random variable $X$ to $\mathbb{R}$, i.e. $p: \mathbb{R_x} \rightarrow \mathbb{R}$.

$$
p_X(x) = P(X=x) = P(\{\omega \in \Omega | X(\omega)=x\})
$$

a proper PMF satisfies the properties

$$
p(x)  \ge 0
$$

and

$$
\sum_{x \in R_x}p(x)=1
$$

## Probability Density Function

Defined as a real-valued **function** from $\mathbb{R}$ to $\mathbb{R}$, i.e. $f: \mathbb{R} \rightarrow \mathbb{R}$

a proper PDF satisfies the following properties

$$
\tag{1} f(x)  \ge 0, \quad \forall x \in \mathbb{R}
$$

and

$$
\int_{-\infty}^{\infty}f(x)dx=1 \tag{2}
$$

**Additional notes (PMF and PDF)**

-   The PMF is defined as the difference between consecutive CDF values
-   The properties of the PMF and PDF show that they are probability measures, as shown by the Axioms of Kolmogorov.

------------------------------------------------------------------------

## Measures of Center and Dispersion

### Expected Value

$$
E[X] = \sum_{x \in R_x} xp(x) \tag{Discrete}
$$

$$
E[X] = \int_{-\infty}^\infty{xf(x)}dx \tag{Continuous}
$$

**Properties of Expected Value**

$$\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]$$

$$\mathbb{E}[c] = c \quad \text{(where } c \text{ is a constant)}$$

$$\mathbb{E}[X_1 + X_2 + \ldots + X_n] = \mathbb{E}[X_1] + \mathbb{E}[X_2] + \ldots + \mathbb{E}[X_n]$$

**Additional Notes**

-   What if the discrete set is infinitely countable? The sum needs to converge absolutely for E(X) to exist.

------------------------------------------------------------------------

### Variance

The variance of a random variable is the expected value of the squared deviation from the mean.

$$
\\ Var(X) = \mathbb{E}([X − \mathbb{E}[X]^2) = \sum_{x \in A}(x-\mu)^2 \tag{Discrete}
p(x)
$$

$$
Var(x) = \int_{-\infty}^{\infty}{\mathbb{E}([X − \mathbb{E}[X]^2)}dx = \int_{-\infty}^{\infty}{\sum_{x \in A}(x-\mu)^2}f(x)dx  \tag{Continuous}
$$

**Shortcut formula for variance**

$$
\mathbb{E}[X^2]-\mathbb{E}[X]^2
$$

**Properties of Variance**

$$\text{Var}(aX + bY) = a^2\text{Var}(X) + b^2\text{Var}(Y)$$

$$\text{Var}(c) = 0 \quad \text{(where } c \text{ is a constant)}$$

**Additional Notes**

-   Properties of variance tend to be derived from expectation

-   It is implicit that expectation of X is defined, however even if E(X) exists it is possible that Var(X) is infinite

### Moments

Moments are used to describe the shape of a random variable distribution, moments are quantitative measures.

-   The first moment is expected value

-   The second central moment is variance

-   The standardized third central moment of X is skew

-   Kurtosis is a measure that is based on the fourth moment and the variance of X.

**Additional Notes**

-   The existence of higher moments implies the existence of lower moments
-   $E[X^n]$ - $n$th moment of X

## Standardized Random Variables

-   Standardization is particularly useful if two or more random variables with different distributions must be compared
-   $X^*$ denotes the standardized random variable X

## Law of the Unconscious Statistician (LOTUS)

The relation between expected value of random variable X and how we can use it to calculate E[g(x)].

$$
E[g(X)] = \sum_{x \in R_x} g(x)p(x)
$$

$$
\mathbb{E}[g(X)] = \int_{-\infty}^{\infty} g(x) \cdot f_X(x) \, dx
$$

**Additional Notes**

-   Implies linearity

## Method of Inverse Transformation

Coming Soon
